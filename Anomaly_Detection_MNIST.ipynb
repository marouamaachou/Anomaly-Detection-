{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjNQqfJcLAVA",
    "outputId": "4100335a-600e-406e-ffeb-2f48f3775bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Maroua/ab_interview\n"
     ]
    }
   ],
   "source": [
    "#%cd drive/MyDrive/Maroua/ab_interview/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkKkDk4B7o-n"
   },
   "source": [
    "# Approach A : GAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7F_EcNy7tjF"
   },
   "source": [
    "## Motivation :\n",
    "Inspired by the dummy approach one can detect the anomalies if we know the true data generating distribution of the non anomalous data by just evaluating the likelihood of a point.The problem with the dummy approach is that we are trying to fit a very complicated distribution that is clearly multimodal(many numbers) with a simple gaussian that is unimodal.\n",
    "\n",
    "We should therefore seek a stronger model able to estimate the distribution of the data manifold .In the GAN training process the discriminator gets better and better at detecting data that is on the manifold from data that is outside the manifold and we could therefore use it to detect the points that are anomalies. The descriminator can therefore be used as a proxy for the likelihood estimation of the manifold. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qnn49SM7-1cb"
   },
   "source": [
    "## Loadings Libs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mrRL-nos7bdH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions import Normal\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from dataset import MnistAnomaly\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(69)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNhSi6XICzu7"
   },
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BWXePAzN-5o5"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "latent_dim = 32\n",
    "\n",
    "\n",
    "# The class for the generator part of the GAN\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 64 * 8, 4, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64 * 2, 64, 4, 2, 2, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output  \n",
    "\n",
    "# The class of the Discriminator part of the GAN\n",
    "class Descriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Descriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64 * 8, 1, 1, 1, 0, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "            # Output: 1\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "# Initizialisation of the weights of the models\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)    \n",
    "\n",
    "# Scoring anomaly of samples using the descriminator\n",
    "def get_scores(des,x_test):\n",
    "    with torch.no_grad():\n",
    "        likelihood = des(x_test.to(device))\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62HFosweC4Wb"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7UdeY3H_MyA",
    "outputId": "6cf6fef7-f0c2-440e-ca12-14069728c538"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:50<00:00, 15.68s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6080182926829267\n",
      "cuda\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:50<00:00, 15.68s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6665394028389623\n",
      "cuda\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:50<00:00, 15.69s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5966293699735148\n",
      "cuda\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:50<00:00, 15.70s/it]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7639928303175145\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:50<00:00, 15.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7037192869296482\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:49<00:00, 15.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7762918101860878\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:49<00:00, 15.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8081386838225142\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:49<00:00, 15.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814721350990804\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:49<00:00, 15.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6988501959431822\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:49<00:00, 15.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7156012967046994\n",
      "roc_auc per digit:\n",
      "['0.608 ', '0.667 ', '0.597 ', '0.764 ', '0.704 ', '0.776 ', '0.808 ', '0.815 ', '0.699 ', '0.716 ']\n",
      "average roc_auc:\n",
      "0.715\n"
     ]
    }
   ],
   "source": [
    "aucs = []\n",
    "\n",
    "for i in range(10):\n",
    "    abnormal_digit = [i]\n",
    "    train_set = MnistAnomaly(\n",
    "        root=\".\", train=True, transform=transforms.ToTensor(), anomaly_categories=abnormal_digit,download=True\n",
    "    )\n",
    "\n",
    "    test_set = MnistAnomaly(\n",
    "        root=\".\", train=False, transform=transforms.ToTensor(), anomaly_categories=abnormal_digit,download=True\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "    batch_size = 2048\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "    print(device)\n",
    "    des = Descriminator().to(device)\n",
    "    gen = Generator().to(device)\n",
    "    des.apply(weights_init)\n",
    "    gen.apply(weights_init)\n",
    "    learning_rate = 0.0002\n",
    "    G_optimizer = torch.optim.Adam(gen.parameters(), lr = learning_rate, betas=(0.5, 0.999))\n",
    "    D_optimizer = torch.optim.Adam(des.parameters(), lr = learning_rate, betas=(0.5, 0.999))\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    for i in tqdm(range(30)):\n",
    "      #epoch = tqdm(range(len(train_loader)))\n",
    "      D_loss_list, G_loss_list = [], []\n",
    "      for batch in range(len(train_loader)):\n",
    "        D_optimizer.zero_grad()\n",
    "        batch_images, _ = next(iter(train_loader))\n",
    "        real_target = Variable(torch.ones(batch_images.size(0),1).to(device))\n",
    "        fake_target = Variable(torch.zeros(batch_images.size(0),1).to(device))\n",
    "        real_des_pred = des(batch_images.to(device)+0.05*torch.randn(batch_images.shape).to(device))\n",
    "        real_des_loss = criterion(real_des_pred,real_target)\n",
    "        real_des_loss.backward()\n",
    "        noise = torch.randn(batch_images.size(0),latent_dim,1,1)\n",
    "        batch_gen_images = gen(noise.to(device))\n",
    "        gen_des_pred = des(batch_gen_images.detach()+0.05*torch.randn(batch_images.shape).to(device))\n",
    "        fake_des_loss = criterion(gen_des_pred,fake_target)\n",
    "        fake_des_loss.backward()\n",
    "        D_total_loss = real_des_loss + fake_des_loss\n",
    "        D_loss_list.append(D_total_loss.item())\n",
    "        D_optimizer.step()\n",
    "        G_optimizer.zero_grad()\n",
    "        gen_output = des(batch_gen_images)\n",
    "        G_loss = criterion(gen_output, real_target)\n",
    "        G_loss_list.append(G_loss.item())\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        total_loss = (G_loss.item()+D_total_loss.item())/2\n",
    "        #epoch.set_postfix({\"epoch\":i,\"train_loss_D \":D_total_loss.item(),\"train_loss_G\":G_loss.item(),\"total loss \":(G_loss.item()+D_total_loss.item())/2})\n",
    "    # test model\n",
    "    test_loader = DataLoader(test_set, batch_size=len(test_set))\n",
    "    x_test, y_test = next(iter(test_loader))\n",
    "\n",
    "    # compute score\n",
    "    score_test = get_scores(des,x_test)\n",
    "\n",
    "    # compute rocauc\n",
    "    roc_auc = roc_auc_score(y_test, score_test.cpu())\n",
    "    print(roc_auc)\n",
    "    aucs.append(roc_auc)\n",
    "print(\"roc_auc per digit:\")\n",
    "print([\"{:0.3f} \".format(auc) for auc in aucs])\n",
    "print(\"average roc_auc:\")\n",
    "print(\"{:0.3f}\".format(torch.tensor(aucs).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vZg4WvI7JIF"
   },
   "source": [
    "# Approach B  : Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWsWpp1TA8Q8"
   },
   "source": [
    "## Motivation : \n",
    "In this approach we use an auto encoder for a different purpose than reconstruction. Indeed , we fit the auto encoder to learn how to reconstruct the normal data and we hope that when encountering an anomalous point the model will struggle to reconstruct it and therefore that will be our clue that the point is an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-zSvK2S2YP3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions import Normal\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from dataset import MnistAnomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89k4R9vKDwG-"
   },
   "source": [
    "## Model implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPspiR0wQ5lR"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.t_conv = nn.ConvTranspose2d(512, 256, 3, stride=1)\n",
    "        self.t_conv1 = nn.ConvTranspose2d(256, 128, 2, stride=1)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(128, 64, 2, stride=1)\n",
    "        self.t_conv3 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.t_conv4 = nn.ConvTranspose2d(32, 1, 2, stride=2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.t_conv(x))\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        x = F.relu(self.t_conv3(x))\n",
    "        x = self.t_conv4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh5TCeym_xIR"
   },
   "outputs": [],
   "source": [
    "def get_scores(autoencoder,x_test):\n",
    "    with torch.no_grad():\n",
    "        reconstructed_test = autoencoder(x_test.to(device))\n",
    "        differences = x_test - reconstructed_test.cpu()\n",
    "        diff_norm = torch.norm(differences,dim=[2,3]).view(-1)\n",
    "    return F.sigmoid(-diff_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_xnBML3Ca2b"
   },
   "source": [
    "## Training :\n",
    "I trained the model for each anomaly class for 400 epochs on a colab pro P100 GPU . \n",
    "\n",
    "PS: i reordered the anomaly cases to start with the problematic ones (1,7,9) to iterate faster on the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cM63eE4cxOnN",
    "outputId": "a9ae75b9-be50-4d41-bca4-4ea5cd0ec4e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [34:05<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3737317719786022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [34:07<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5636228674440324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [34:12<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7007698290921518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [34:12<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9290432519760181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [34:32<00:00,  5.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8408352514895538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [34:23<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7850067007871562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [34:22<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8458249388011524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [34:25<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9216806722883097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [34:24<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8556437574135591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [34:22<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1325)\n",
      "0.8625040725824697\n",
      "roc_auc per digit:\n",
      "['0.374 ', '0.564 ', '0.701 ', '0.929 ', '0.841 ', '0.785 ', '0.846 ', '0.922 ', '0.856 ', '0.863 ']\n",
      "average roc_auc:\n",
      "0.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def compress(x):\n",
    "    return x.reshape(len(x), -1).sum(-1)\n",
    "\n",
    "aucs = []\n",
    "ordered = [1,9,7,2,3,4,5,6,8,0]\n",
    "for i in ordered:\n",
    "    abnormal_digit = [i]\n",
    "    train_set = MnistAnomaly(\n",
    "        root=\".\", train=True, transform=transforms.ToTensor(), anomaly_categories=abnormal_digit,download=True\n",
    "    )\n",
    "\n",
    "    test_set = MnistAnomaly(\n",
    "        root=\".\", train=False, transform=transforms.ToTensor(), anomaly_categories=abnormal_digit,download=True\n",
    "    )\n",
    "    train_loader = DataLoader(train_set, batch_size=len(train_set))\n",
    "    x, _ = next(iter(train_loader))\n",
    "    x = compress(x)\n",
    "    sd,m = x.std(),x.mean()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "    batch_size = 2048\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "\n",
    "    autoencoder = AutoEncoder().to(device)\n",
    "    optimizer = AdamW(params=autoencoder.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for _ in tqdm(range(100)):\n",
    "      #epoch = tqdm(range(len(train_loader)))\n",
    "      epoch = range(len(train_loader))\n",
    "      for batch in epoch:\n",
    "        batch_images, _ = next(iter(train_loader))\n",
    "        #reconstructed_images = autoencoder.forward(batch_images.to(device)+(sd*torch.randn(batch_images.shape)+m).to(device))\n",
    "        reconstructed_images = autoencoder.forward(batch_images.to(device))\n",
    "        loss = criterion(batch_images.to(device),reconstructed_images)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #epoch.set_postfix({\"train_loss : \":loss.item()})\n",
    "\n",
    "    # test model\n",
    "    test_loader = DataLoader(test_set, batch_size=len(test_set))\n",
    "    x_test, y_test = next(iter(test_loader))\n",
    "\n",
    "    # compute score\n",
    "    score_test = get_scores(autoencoder,x_test)\n",
    "\n",
    "    # compute rocauc\n",
    "    roc_auc = roc_auc_score(y_test, score_test)\n",
    "    print(roc_auc)\n",
    "    aucs.append(roc_auc)\n",
    "print(\"roc_auc per digit:\")\n",
    "print([\"{:0.3f} \".format(auc) for auc in aucs])\n",
    "print(\"average roc_auc:\")\n",
    "print(\"{:0.3f}\".format(torch.tensor(aucs).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z08FxipBI8Cp"
   },
   "source": [
    "# Results: \n",
    " We see that the approach B achieves very interesting results (>80 AUC) on many anomaly cases.But we see that the approach finds some difficulties on some anomaly cases especially the 1 and 9 cases.\n",
    "\n",
    " For the 1 case , it is quite a surprise that the AUC is far lower than 0.5 meaning that inverting our classification scheme would lead to a better result.One explanation that i found by investgating the recontructions is that it easier to reconstruct the number one as it is mainly as straight line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boMjwtCZN-Cn"
   },
   "source": [
    "## Ideas to explore :\n",
    "\n",
    "* trying to train a Normalizing Flow. This approach is similar to the GAN as i would train a generative model to generate my normal data but the good part in Normalizing Flows is that is allows exact likelihood computing through the chained change of variable formula. Calculating this likelihood would allow to differentiate the normal from anomalous data.\n",
    "* Trying a Siamese Convolutional Network trained on the \"normal classes\" and missing the anomalous one. When spotting this class, identifying it as an unknown/new one.\n",
    "* A paper i wanted to explore and test if i had more time : https://arxiv.org/pdf/2004.07657v4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Anomaly Detection MNIST Maachou.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
